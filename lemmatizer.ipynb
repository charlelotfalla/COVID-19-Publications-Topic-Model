{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This function is for stop-word removal and lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(doc, min_df, max_df, dtm_flag):\n",
    "\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import re, nltk, spacy\n",
    "    import pickle\n",
    "    import os\n",
    "    import scispacy\n",
    "    from spacy import displacy\n",
    "    from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    import gensim\n",
    "    from gensim import corpora, models\n",
    "    from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n",
    "    from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "\n",
    "    import en_core_sci_lg\n",
    "    \n",
    "    ### Load Scispacy model\n",
    "    nlp = en_core_sci_lg.load()\n",
    "    \n",
    "    ### Lower case column names and drop NA\n",
    "    docs = doc \n",
    "    docs.columns = map(str.lower, docs.columns)\n",
    "    docs = docs[['pubmedid', 'abstract']]\n",
    "    docs = docs.dropna()\n",
    "    \n",
    "    abstracts = docs['abstract'].astype('str')\n",
    "    abstracts = abstracts.tolist()\n",
    "    \n",
    "    ### Remove new line characters and extra space\n",
    "    data = [re.sub('\\s+', ' ', abstract) for abstract in abstracts]\n",
    "\n",
    "    ### Define custom stopwords for biomedical analysis and any contextual words\n",
    "    custom_stop = list(\"\"\"\n",
    "    et al www com patient study mg\n",
    "    \"\"\".split())\n",
    "\n",
    "    ### Add to existing stopword dictionary\n",
    "    for word in custom_stop:\n",
    "        nlp.vocab[word].is_stop = True\n",
    "\n",
    "    ### Lemmatization\n",
    "    data_lemma = []\n",
    "    for txt in tqdm(abstracts):\n",
    "        lis = []\n",
    "        doc = nlp(txt)\n",
    "        for token in doc:\n",
    "            lis.append(token.lemma_)\n",
    "        data_lemma.append(' '.join(lis))\n",
    "    \n",
    "\n",
    "    def tokenization_with_gen_stop(text):\n",
    "        result=[]\n",
    "        for token in gensim.utils.simple_preprocess(text) :\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "                gensim.parsing.preprocessing.strip_punctuation(token)\n",
    "                result.append(token)\n",
    "        return result\n",
    "\n",
    "    ## Apply tokenization function\n",
    "    data_words = []\n",
    "    for txt in tqdm(data_lemma):\n",
    "        data_words.append(tokenization_with_gen_stop(txt))\n",
    "        \n",
    "   \n",
    "    ### NLTK Stopword removal (extra stopwords)\n",
    "\n",
    "    data_words_clean = []\n",
    "    for word in tqdm(data_words):\n",
    "        wrd = []\n",
    "        for w in word:\n",
    "            if w not in STOP_WORDS:\n",
    "                wrd.append(w)\n",
    "        data_words_clean.append(wrd)\n",
    "    \n",
    "    ### Create dictionary and corpus required for Topic Modeling\n",
    "    \n",
    "    dictionary = corpora.Dictionary(data_words_clean)\n",
    "    dictionary.filter_extremes(no_below=min_df, no_above=max_df)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data_words_clean]\n",
    "    \n",
    "    if (dtm_flag == 1):\n",
    "        tfidf = models.TfidfModel(corpus, normalize = True)\n",
    "        corpus = tfidf[corpus]\n",
    "    elif (dtm_flag == 0):\n",
    "        corpus = corpus\n",
    "    \n",
    "    return corpus, dictionary, data_words_clean, docs['pubmedid']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
